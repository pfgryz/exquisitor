\documentclass{article}

\usepackage{xcolor}
\usepackage{tcolorbox}     % Color boxes
\usepackage{amsmath}       % Basic mathematical typesetting
\usepackage{amssymb}       % Advanced math symbols
\usepackage{amsthm}        % Theorems typesetting
\usepackage{array}         % Advanced table column formats
\usepackage{enumitem}      % Itemize/enumrate
\usepackage{fancyhdr}      % Custom header/footer styles
\usepackage{fourier}       % Adobe Utopia font
\usepackage{graphicx}      % Enhanced images support
\usepackage{ifluatex}      % LuaTeX-specific options
\usepackage{kantlipsum}    % English kantian-style lipsum
\usepackage{lipsum}        % Lorem ipsum
\usepackage{listings}      % Code listings
\usepackage{longtable}     % Multi-page tables
\usepackage{multirow}      % Advanced table cells
\usepackage{setspace}      % Set space between lines
\usepackage{scrextend}     % Allows \addmargin environment
\usepackage{tablefootnote} % Table footnotes
\usepackage{tocloft}       % Custom ToC/LoF/LoT
\usepackage{url}           % URL-sensitive line breaks
\usepackage{xspace}        % Remove duplicated spaces
\usepackage{tabularx}

\setstretch{1.15}
\setlength{\parindent}{5mm}
\counterwithin{equation}{section}
\counterwithin{figure}{section}
\counterwithin{table}{section}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

% List formatting
\setlist[itemize,1]{topsep=2pt,label=\large$\bullet$, leftmargin=28pt}
\setlist[itemize,2]{topsep=2pt,leftmargin=18pt}
\setlist[itemize,3]{topsep=2pt,leftmargin=18pt}
\setlist[enumerate,1]{topsep=2pt,leftmargin=24pt}
\setlist[enumerate,2]{topsep=2pt,leftmargin=16pt}
\setlist[enumerate,3]{topsep=2pt,leftmargin=16pt}

% TabularX columns
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}

% region LuaLatex Scripts

% Script from: https://github.com/ArturB/WUT-Thesis
\usepackage{ifluatex}      % LuaTeX-specific options
\ifluatex
    \usepackage[T1]{fontenc}
    \usepackage[utf8]{luainputenc}
    \usepackage{luacode}
    % In LuaTeX, we can prevent one-letter words
    % from beging at the end of the line.
    \begin{luacode}
    local debug = false
    local glyph_id = node.id "glyph"
    local glue_id  = node.id "glue"
    local hlist_id = node.id "hlist"

    local prevent_single_letter = function (head)
        while head do
            if head.id == glyph_id then
                if unicode.utf8.match(unicode.utf8.char(head.char),"%a") then     -- is a letter
                    if head.prev.id == glue_id and head.next.id == glue_id then   -- is one letter word

                        local p = node.new("penalty")
                        p.penalty = 10000

                        if debug then
                            local w = node.new("whatsit","pdf_literal")
                            w.data = "q 1 0 1 RG 1 0 1 rg 0 0 m 0 5 l 2 5 l 2 0 l b Q"
                            node.insert_after(head,head,w)
                            node.insert_after(head,w,p)
                        else
                            node.insert_after(head,head,p)
                        end
                    end
                end
            end
            head = head.next
        end
        return true
    end

    luatexbase.add_to_callback("pre_linebreak_filter",prevent_single_letter,"~")
    \end{luacode}
\fi

% endregion

% region Custom Tags

\newcommand{\temporary}[1]{
    \begin{tcolorbox}[colframe=red, colback=white, title={\textbf{WERSJA PO POLSKU}}, sharp corners=south]
        #1
    \end{tcolorbox}
}

% endregion

\begin{document}

    \title{Title of article}
    \author{Patryk Filip Gryz}

    \maketitle

    \textbf{Abstract}
    The discovery of DNA revolutionized biology and medicine, enabling molecular research and disease diagnosis. Key milestones include the 1953 discovery of the DNA double helix by Watson and Crick, and the 1977 sequencing of bacteriophage X174, which highlighted the need for computational analysis of DNA sequences. As sequencing costs decreased with next-generation methods, computational demands increased. This study explores using artificial neural networks (ANN) for taxonomic classification of large DNA sequence datasets. The proposed ANN model identifies representative sequences for classification, reducing computational complexity while maintaining accuracy. Experiments show that the ANN model improves classification quality and efficiency compared to traditional methods. This research demonstrates the potential of ANNs to enhance taxonomic classification, reducing time and increasing result quality. Future work may involve integrating ANNs for dimensionality reduction in genetic sequence analysis, further advancing bioinformatics.

    \clearpage
    \section{Introduction}

        \subsection{Background}

            The discovery of DNA marked the beginning of a new era in biology and medicine, enabling research into the molecular foundations of life and the precise diagnosis of many diseases\cite{Louie:2000}. In 1953, the structure of the DNA double helix was discovered by Watson and Crick\cite{Watson:1953}. In 1970, Francis Crick formulated the central dogma of molecular biology\cite{Crick:1970}, which states that genetic information flows from DNA to RNA and then to proteins, forming the foundation of modern molecular biology. In 1977, the complete genetic material of the DNA virus â€” bacteriophage $\phi{}$X174 was sequenced\cite{Sanger:1977:2}. This discovery highlighted the limitations of traditional analysis methods and demonstrated the need for computers to process DNA sequences\cite{Staden:1979}. In the 1980s, the European "EBML Data Library" was funded\cite{Higgins:1992}, and in 1982, the United States launched "GenBank"\cite{Bilofsky:1986}. In 1998, Jo Handelsman proposed the analysis of genetic material from many organisms simultaneously, without the need for cultivation, with his team studying organisms present in soil\cite{Handelsman:1998}. The use of information about genetic material found in the environment, combined with sequence databases, enables the identification of organisms present in a given sample. However, the process of analyzing large numbers of DNA sequences and comparing them to databases requires significant computational resources. With the development of next-generation sequencing methods\cite{Reinartz:2002}, sequencing costs have decreased significantly, and the number of DNA sequences being analyzed has increased\cite{Muir:2016}. Further accumulation of sequence data and increasing sequencing throughput lead to a growing demand for computational resources.
        
        \subsection{Motivation}

            The time required for taxonomic classification of large sequence datasets using traditional tools, such as BLAST, can span several hours, even for relatively small collections of sequences e.g. 4096 sequences. In many fields, artificial neural networks (ANN) have demonstrated the ability to outperform classical heuristic algorithms, offering faster and more accurate results. By leveraging the capabilities of ANN, this works aims to accelerate taxonomic clasisfication process while maintaining at least the same level of quality as traditional methods.

        \subsection{Objectives}
    
            The primary objective of this study is to leverage an artificial neural network to support the selection of representative genetic sequences for taxonomic classification. Instead of classifying entire environmental genetic sequences sample, the proposed approach focuses on identifying a subset of sequences that best represent the sample. These selected representatives are then classified, reducing computational complexity while maintaining classification accuracy.

        \subsection{Main Contributions}

            This paper introduces a novel artificial neural network model for calculating dissimilarities between genetic sequences, which can be used in clustering algorithm to select representative sequences for taxonomic classification. We provide all the necessary components for training the model, including dataset preparation, batch process, a loss function, and a training loop. The paper also includes an application that can perform taxonomic classification with specified parameters. Furthermore, we develope a library that allows users to compose the taxonomic classification pipelines.

        \subsection{Structure of the Paper}

            The rest of the paper is organized as follows. Section 2 provides a description of related work, reviewing previous research on taxonomic classification and highlights a research gap. Section 3 defines the problem, presents the proposed approach, and provides detailed implementation details. Section 4 describes the experimental setup, including used datasets, the performance metrics for evaluation, and the results obtained from experiments with analysis. Section 5 interprets the results, discusses the limtiations of proposed approach, and explores possible improvements for future work. Finally, Section 6 concludes the paper by summarizing the main findings and suggesting potential directions for future research.

    \clearpage
    \section{Related Work}

        \subsection{Overview of Existing Approaches}

            Historically, one of the first algorithms that enabled taxonomic classification was the Needleman-Wunsch algorithm \cite{Needleman:1970}, developed in 1970, which allowed for the comparison of genetic sequences. However, the first solution that enabled taxonomic classification within a reasonable time frame was a tool created in 1983 by D. Lipman and W. Wilbur \cite{Wilbur:1983}. This method was based on dividing sequences into $k$-tuples, a generalization of $k$-mers, and comparing them. An advancement of this approach was the BLAST algorithm, introduced in 1990 \cite{Altschul:1990}, which also relied on $k$-mers and allowed for the efficient search of similar sequences in NCBI databases.

            The most significant progress in research, however, has occurred over the past 15 years. A notable example is the development of the MetaPhlAn tool in 2012 \cite{Segata:2012}, which uses marker genes to compare the species composition of metagenomic samples. Another marker gene-based approach is mOTUs2 \cite{Milanese:2019}, which identifies and analyzes markers unique to specific microbial strains. A different technique was employed in Centrifuge \cite{Kim:2016}, developed in 2016, which utilizes sequence indexing based on the Burrows-Wheeler transform \cite{Burrows:1994} and the Ferragina-Manzini index \cite{Ferragina:2000} for efficient sequence retrieval.
        
            A more classical approach is used in Kraken \cite{Wood:2014}, introduced in 2014, which combines $k$-mers with an indexed sequence database. An unconventional method for taxonomic classification was proposed in 2022 with BERTax \cite{Mock:2022}, which applies a transformer model \cite{Transformers} to DNA sequence analysis, treating DNA as a specialized language and allowing for classification without the need for reference databases. Another machine learning-based solution is CGRclust \cite{Alipour:2024}, which clusters DNA sequences using a two-dimensional chaos game representation and integrates unsupervised contrastive learning with convolutional neural networks.

        \subsection{Comparative Analysis}

            Algorithms based on $k$-mers are very fast, but they do not take into account all the information about the structure of the input genetic sequences, which reduces their quality. Solutions using marker genes are characterized by good quality, but they allow analysis only for subsets of organisms for which marker genes are known. Alignment-based tools should yield the best results, but they require significant computational resources.

        \subsection{Research Gaps}

            Although recent years have seen studies on taxonomic classification using machine learning methods, there is still a lack of research focusing on the application of artificial neural networks with contrastive learning for efficient sequence clustering. Such an approach could significantly accelerate the classification process using available tools.

    \clearpage
    \section{Methodology}

        \subsection{Proposed Approach}

            The proposed method involves reducing the dimensionality of input DNA sequences to a feature vector in the form of $\mathbb{R}^{64}$ using an artificial neural network. The neural network employs contrastive learning, which enables the learning of representations while preserving dissimilarity between sequences. The dissimilarity between sequence representations will be calculated using cosine dissimilarity, expressed by the formula in equation \eqref{Equation:CosineDissimilarity}.

            Additionally, two classical approaches will be implemented in the work, which will be used for comparison with the neural network.

            \subsubsection{Modified Needleman-Wunshc Algorithm}

                The first classical approach is the Needleman-Wunsch algorithm. This algorithm allows for global alignment of genetic sequences. It uses a similarity matrix between sequences, which is constructed according to the rules given in equation~\eqref{Equation:NeedlemanWunsch}. A modification of the approach presented in equation~\eqref{Equation:NeedlemanWunschModified} is applied in the work.

                \begin{equation}
                    \begin{aligned}
                        D_{i,0} &= i \cdot g, & \text{dla } & i \in [1, n + 1] \\
                        D_{0,j} &= j \cdot g, & \text{dla } & j \in [2, m + 1] \\
                        D_{i,j} &= \max
                        \begin{cases}
                            D_{i - 1, j} + g \\
                            D_{i, j - 1} + g \\
                            D_{i - 1, j - 1} + s(A_i, B_j)
                        \end{cases}, & \text{dla } & i \in (1, n + 1] \text{ oraz } j \in (1, m + 1]
                    \end{aligned}
                    \label{Equation:NeedlemanWunsch}
                \end{equation}
    
                where:
                \begin{align*}
                    A, B -& \text{compared sequences}, \\
                    n, m -& \text{lengths of sequences } A \text{ and } B, \\
                    D -& \text{similarity matrix of size } n + 1 \text{ x } m + 1, \\
                    g \in \mathbb{R} -& \text{penalty for a gap}, \\
                    s(A_i, B_j) \in \mathbb{R} -& \text{similarity between the } i\text{-th element in sequence A and the } j\text{-th element in sequence B}.
                \end{align*}

                \begin{equation}
                    \begin{aligned}
                        D_{i,j} &= \min
                        \begin{cases}
                        D_{i - 1, j} + g \\
                        D_{i, j - 1} + g \\
                        D_{i - 1, j - 1} + s(A_{i - 1}, B_{j - 1})
                        \end{cases}, & \text{dla } & i \in \left(1, n + 1\right] \text{ oraz } j \in \left(1, m + 1\right] \\
                    \end{aligned}
                    \label{Equation:NeedlemanWunschModified}
                \end{equation}
    
                In the solution, the following parameter values were adopted:
                \begin{align*}
                    g &= 2, \\
                    s(a, b) &= \begin{cases}
                        0, & \text{dla } a = b, \\
                        1, & \text{dla } a \neq b.
                    \end{cases}
                \end{align*}

            \subsubsection{$k$-mer embeddings}

                The second approach will be the use of $k$-mers as embeddings, which allows for the representation of DNA sequences as numerical vectors. These vectors will then be compared using Euclidean distance.
                
            \subsubsection{Artificial Neural Network}

                \paragraph{Architecture}
                The ANN model consists of two parts. The first part comprises two convolutional blocks with batch normalization and is responsible for feature extraction from the sequence. The second part utilizes fully connected layers with dropout and the GELU activation function \cite{Hendrycks:2016} and is connected to the first part of the model via a flattening layer.

                The model output is the output of the final linear layer, which is a feature vector of dimension \( \mathbb{R}^{64} \).

                Schematically, the architecture is presented in Figure~\ref{Picture:NeuralModel}.

                \begin{figure}[!htb]
                    \begin{center}
                        {
                        % ===== BEGIN =====
                        % ----- -----
                        % COLORS
                        % ----- -----
                        \definecolor{Green}{HTML}{1dd1a1}   % Input
                        \definecolor{Blue}{HTML}{54a0ff}    % Linear
                        \definecolor{Yellow}{HTML}{feca57}  % Convolution
                        \definecolor{Purple}{HTML}{5f27cd}  % Batch Norm
                        \definecolor{Grey}{HTML}{576574}    % Dropout
                        \definecolor{Red}{HTML}{ff6b6b}     % Output
                        \definecolor{Pink}{HTML}{ff9ff3}    % Activation
                        \definecolor{Background}{HTML}{c8d6e5}

                        % ----- -----
                        % ELEMENTS
                        % ----- -----
                        \tikzstyle{box} = [rectangle, rounded corners, minimum width=5cm, minimum height=1cm, text centered, draw=black, align=center]
                        \tikzstyle{input} = [box, fill=Green!30]
                        \tikzstyle{linear} = [box, fill=Blue!30]
                        \tikzstyle{conv} = [box, fill=Yellow!30]
                        \tikzstyle{bn} = [box, fill=Purple!30]
                        \tikzstyle{activation} = [box, fill=Pink!30]
                        \tikzstyle{dropout} = [box, fill=Grey!30]
                        \tikzstyle{output} = [box, fill=Red!30]

                        \tikzstyle{arrow} = [very thick, -Triangle]
                        \tikzstyle{arrow:text} = [pos=0.5, right, font=\footnotesize]

                        % ----- -----
                        % PICTURE
                        % ----- -----
                        \begin{tikzpicture}[node distance=2cm]
                            \node (input) [input] { WejÅ›cie };
                            \node (conv1) [conv, below of=input] { Splot 1D \\ \textbf{16@1x16, krok: 4} };
                            \node (bn1) [bn, below of=conv1] { Normalizacja wsadowa };
                            \node (conv2) [conv, below of=bn1] { Splot 1D \\ \textbf{32@1x8} };
                            \node (bn2) [bn, below of=conv2] { Normalizacja wsadowa };
                            \node (flatten) [linear, below of=bn2] { Warstwa spÅ‚aszczajÄ…ca };
                            \node (flatten-right) [right of=flatten, xshift=2cm] {};

                            \node (fc1-left) [right of=input, xshift=2cm] {};
                            \node (fc1) [linear, right of=input, xshift=6cm] { Warstwa gÄ™sta };
                            \node (act1) [activation, below of=fc1] { Aktywacja \\ \textbf{GELU} };
                            \node (drop1) [dropout, below of=act1] { WyÅ‚Ä…czenie neuronÃ³w };
                            \node (fc2) [linear, below of=drop1] { Warstwa gÄ™sta };
                            \node (act2) [activation, below of=fc2] { Aktywacja \\ \textbf{GELU} };
                            \node (drop2) [dropout, below of=act2] { WyÅ‚Ä…czenie neuronÃ³w };
                            \node (fc3) [linear, below of=drop2] { Warstwa gÄ™sta };
                            \node (output) [output, below of=fc3] { WyjÅ›cie };

                            \draw [arrow] (input) -- (conv1) node [arrow:text] {1x600};
                            \draw [arrow] (conv1) -- (bn1) node [arrow:text] {16x147};
                            \draw [arrow] (bn1) -- (conv2) node [arrow:text] {16x147};
                            \draw [arrow] (conv2) -- (bn2) node [arrow:text] {32x140};
                            \draw [arrow] (bn2) -- (flatten) node [arrow:text] {32x140};

                            \draw [arrow] (flatten.east) -- (flatten-right) -- node [arrow:text] {1x4480} (fc1-left) -- (fc1.west);
                            \draw [arrow] (fc1) -- (act1) node [arrow:text] {1x4096};
                            \draw [arrow] (act1) -- (drop1) node [arrow:text] {1x4096};
                            \draw [arrow] (drop1) -- (fc2) node [arrow:text] {1x4096};
                            \draw [arrow] (fc2) -- (act2) node [arrow:text] {1x512};
                            \draw [arrow] (act2) -- (drop2) node [arrow:text] {1x512};
                            \draw [arrow] (drop2) -- (fc3) node [arrow:text] {1x512};
                            \draw [arrow] (fc3) -- (output) node [arrow:text] {1x64};
                        \end{tikzpicture}

                        % ===== END =====
                        }
                    \end{center}
                    \caption{
                        Diagram of ANN architecture.
                    }\label{Picture:NeuralModel}
                \end{figure}

                \paragraph{Input}
                The input to the model consists of DNA sequences of length $150$, which are encoded into a vector of dimensions $1 \times 600$ using one-hot encoding \cite{HarrisDavid:2007}.

                \paragraph{Training Examples}
                The training and validation examples consist of an anchor, a positive sequence that is similar to the anchor, and a negative sequence that is dissimilar to the anchor.

                \paragraph{Dataset}
                ZbiÃ³r danych uczÄ…cych oraz walidacyjnych zostaÅ‚ stworzony na podstawie pierwszej prÃ³bki sekwencji genetycznych ze zbioru \textit{CAMI II Toy Human Microbiome Project}\cite{Fritz:2019}. PrÃ³bka zawiera symulowane dane metagenomiczne z mikrobiomu skÃ³ry czÅ‚owieka. PrzykÅ‚ady zostaÅ‚y uzyskane poprzez losowanie kotwic ze zbioru oraz modyfikacjÄ™ tych kotwic w celu stworzenia sekwencji pozytywnej i negatywnej. Modyfikacja polegaÅ‚a na zamianie punktowej danego nukleotydu na inny. W przypadku sekwencji pozytywnej zmiana obejmowaÅ‚a od $0\%$ do $20\%$ dÅ‚ugoÅ›ci kotwicy, natomiast w przypadku sekwencji negatywnej od $20\%$ do $80\%$ dÅ‚ugoÅ›ci kotwicy.

                \paragraph{Loss function}
                The loss function used is defined as:

                \begin{equation}
                    \text{Strata kontrastowa} = [m_{pos} - s_{pos}]_{+} + [s_{neg} - m_{neg}]_{+}
                \end{equation}

                where:
                \begin{align*}
                    m_{pos} &- \text{similarity margin between the positive sequence and the anchor,} \\
                    m_{neg} &- \text{similarity margin between the negative sequence and the anchor,} \\
                    s_{pos} &- \text{cosine similarity of the positive sequence to the anchor,} \\
                    s_{pos} &- \text{cosine similarity of the negative sequence to the anchor.} \\
                \end{align*}
                
                \paragraph{Learning}
                The model was trained on a dataset of $10^{6}$ training examples and $10^{4}$ validation examples. The optimizer used was \textit{AdamW}\cite{Loshchilov2017DecoupledWD}.

                \paragraph{Quality}
                As a measure of model quality, the contrastive loss of the model was computed on the validation set.

                \paragraph{Parameters}
                Experiments were conducted to determine the optimal training parameters for the artificial neural network model. The tested parameters included the learning rate coefficient \( \lambda \), weight decay \( w \), the coefficient \( \gamma \) used in the exponential learning rate decay, the dropout rate, and the effectiveness of using three perceptron layers.

                As a result of these experiments, the best parameters were selected:
                \[
                \lambda = 10^{-6}, \quad w = , \quad \gamma = 0.99999
                \]
                a dropout rate of \( 0.5 \), and a confirmed positive impact of using three perceptron layers.

                Additionally, the loss function was used with the following parameters:
                \[
                m_{pos} = 1.0, \quad m_{neg} = 0.25.
                \]


                \paragraph{Results of Learning}
                As a result of training the neural network, the model achieved a validation set loss of $0.17$.

        \subsection{Implementation Details}

            The work used the Python programming language\cite{Python} in scripts that automate some tasks related to neural network learning, and the Rust programming language\cite{Rust}, which was used to implement the solution.

            The solution consists of two components. The first is a library that contains components enabling the creation of pipelines for genetic material analysis in the form of taxonomic classification. The second component is a console application, which uses the library's components to allow for the creation and configuration of processing pipelines. Additionally, a web application was created, which allows users to submit analysis tasks through a web browser.  

            The console application was implemented using the \textit{clap} library\cite{Rust:clap}. In library the $k$-medoid algorithm was provided by the \textit{kmedoids} library\cite{Rust:kmedoids}, and the neural network was built using the \textit{burn} library\cite{Rust:burn} with the \textit{wgpu} execution environment. Web application was implemented using the \textit{axum} library\cite{Rust:axum} with \textit{tokio}\cite{Rust:axum} async environment.
                
            Additionally, the following tools were used in the work:
            \begin{itemize}
                \item \textit{cargo} as the package manager and build system in Rust;
                \item \textit{rustup} for automatic management of Rust versions;
                \item \textit{clippy} for static code analysis in Rust;
                \item \textit{rustfmt} for automatic formatting of Rust source code;
                \item \textit{cargo test} for conducting unit tests;
                \item \textit{git} as the version control system, enabling change tracking and code history management.
            \end{itemize}
        
    \clearpage
    \section{Experiments and Results}

        \subsection{Experimental Setup}

            Experiments were conducted on the KVM based virtual machine with the specification given below:

            \begin{itemize}
                \item{
                    \textbf{Operating System:} Ubuntu 22.04 LTS;
                }
                \item{
                    \textbf{Processor:} 4 virtual cores of Intel Core i7-6850K;
                }
                \item{
                    \textbf{RAM Memory:} 40 GB;
                }
                \item{
                    \textbf{Graphics Card:} Nvidia GeForce GTX 1080 TI;
                }
                \item{
                    \textbf{Disk:} 1 TB network drive with a read speed of 1 Gbps;
                }
                \item{
                    \textbf{Software:} \texttt{BLAST} package version 2.16.0 and graphics card drivers.
                }
            \end{itemize}

        \subsection{Datasets}

            \subsubsection{Dataset Description}

                In the experiments, the \textit{CAMI II Toy Human Microbiome Project}\cite{Fritz:2019} dataset was used, which is the same dataset used for training the artificial neural network model. Dataset was chosen, because it was created for benchmarking bioinformatics tools and contains a large number of sequences, enabling its use both in experiments and in the learning process.

                Subsets of sequences were created from the dataset with sizes expressed by the formula $2^k$ for $k \in [0, 12]$. Subsets are disjoint, and only those sequences that were not used for training the ANN model were employed in their construction.

            \subsubsection{Dataset Preparation}

                The subsets were created by randomly sampling, without replacement, the indices of genetic sequences from the reference dataset that were to be included in each subset. Indices of sequences used in the training and validation sets of the ANN were excluded from the sampling. 

        \subsection{Performance Measures}

            Quality of taxonomic classification was measured using a modified Jaccard index, expressed by equation~\ref{Equation:Quality}. The measure was used to compare the quality of taxonomic classification performed using the implemented methods against taxonomic classification without the use of a processing pipeline.

            \begin{equation}
                \text{Q} = \frac{
                    \sum_{r \in (O(R) \cap O(E))} R_{r}
                }{
                    \sum_{r \in (O(R) \cup O(E))} R_{r}
                }
                \label{Equation:Quality}
            \end{equation}

            where:
            \begin{align*}
                R &- \text{set of reference results,} \\
                E &- \text{set of obtained results,} \\
                R_{r} &- \text{number of results in reference set assigned to the organism $r$} \\
                O(X) &- \text{set of unique organisms, for which results were assigned in the set $X$}
            \end{align*}

            To compare the quality of multiple taxonomic clasisfication runs, the weighted average quality, defined by equation~\ref{Equation:WeightedAverageQuality}, was used.
            
            \begin{equation}
                Q_{\text{avg}} = \sum_{c \in C} \frac{n_c}{n} Q_c
                \label{Equation:WeightedAverageQuality}
            \end{equation}
    
            where:
            \begin{align*}
              C &- \text{set of taxonomic classification runs,} \\
              Q_c &- \text{quality of taxonomic classification $c$,} \\
              n_c &- \text{number of input sequences for taxonomic classification $c$,}\\
              n   &- \text{number of input sequences $n = \sum_{c \in C} n_{c}.$}
            \end{align*}

        \subsection{Results and Analysis}

            \subsubsection{Experiment 1: Taxonomic classification execution time}

                \paragraph{Objective} 
                Measuring the execution time of taxonomic classification using different genetic sequence clustering methods.

                \paragraph{Assumptions}
                \begin{enumerate}
                    \item {
                        Sequences clustering is deterministic.
                    }
                    \item {
                        Taxonomic classification is the only task running on the machine.
                    }
                \end{enumerate}

                \paragraph{Results}
                As a result of the experiment, the execution time for taxonomic classification was obtained for all methods. In the case of the method using the modified Needleman-Wunsch algorithm, results for taxonomic classification of 4096 sequences could not be obtained due to exceeding the execution time. Figure~\ref{Picture:Experiment:Duration} shows a graph of the taxonomic classification execution time as a function of the number of input sequences for: the method with the modified Needleman-Wunsch algorithm (NW), the method using $k$-mer embeddings ($k$-mer), the method using artificial neural networks (ANN), and for taxonomic classification of all sequences without using a pipeline (NP). Detailed execution times are provided in Table~\ref{Table:Experiment:Duration}.

                \begin{figure}[!htb]
                    \begin{center}
                        \includegraphics[width=\textwidth]{pictures/experiment_duration.png}
                    \end{center}
                    \caption{
                        Taxonomic classification execution time.
                    }\label{Picture:Experiment:Duration}
                \end{figure}

                \begin{table}\centering
                    \caption{Taxonomic classification execution time.}\label{Table:Experiment:Duration}
                    \begin{tabularx}{\textwidth}{|c||R|R|R|R|}
                        \hline
                        \multirow{2}{*}{\textbf{Number of sequences}} & \multicolumn{4}{|c|}{\textbf{Execution time [s]}} \\ \cline{2-5}
                                        & \textbf{NP} & \textbf{NW} & \textbf{$k$-mer} & \textbf{ANN} \\ \hline \hline
                                        1 & 7107 & \textbf{6025} & 6087 & 6077\\ \hline
                                        2 & 6129 & 5994 & \textbf{5988} & 6035\\ \hline
                                        4 & 6108 & \textbf{5983} & 6024 & 6019\\ \hline
                                        8 & 6196 & 5988 & \textbf{5925} & 6014\\ \hline
                                        16 & 6290 & \textbf{5941} & 5946 & 6118\\ \hline
                                        32 & 6279 & 6035 & \textbf{5962} & 6092\\ \hline
                                        64 & 6316 & 6113 & \textbf{6107} & 6139\\ \hline
                                        128 & 6560 & 6238 & \textbf{6206} & 6233\\ \hline
                                        256 & 6753 & 6446 & \textbf{6295} & 6363\\ \hline
                                        512 & 7091 & 6972 & \textbf{6326} & 6347\\ \hline
                                        1024 & 8059 & 8743 & \textbf{6248} & 6269\\ \hline
                                        2048 & 8517 & 16461 & 6472 & \textbf{6332}\\ \hline
                                        4096 & 9433 & \textbf{-1} & 7003 & 6550\\ \hline

                    \end{tabularx}
                \end{table}

                \paragraph{Conclusions}
                The method using the modified Needleman-Wunsch algorithm exhibited the fastest increase in execution time for taxonomic classification. The sharp rise is due to the time-consuming process of comparing sequences with each other. Despite reducing the number of sequences being classified, the execution time using this method exceeded that of taxonomic classification for all sequences.

                The methods using $k$-mer embeddings and ANN showed comparable execution times, with a slight advantage for the former. Both methods use sequence representations for comparison, resulting in a slower increase in the time needed to build the dissimilarity matrix. These methods performed faster than taxonomic classification for all sequences. In the case of the ANN, the shorter execution time for $4096$ sequences could be due to the simultaneous computation of embeddings for all sequences using the GPU.

            \subsubsection{Experiment 2: Taxonomic classification quality}

                \paragraph{Objective}
                Examining the quality of taxonomic classification using the implemented methods in comparison to the taxonomic classification of all sequences.

                \paragraph{Assumptions}
                \begin{enumerate}
                    \item {
                        Sequences clustering is deterministic.
                    }
                    \item {
                        \texttt{BLASTn} is deterministic and always returns the same results for a given sequence and specified parameters.
                    }
                    \item {
                        In the case where it is not possible to compute the quality for a given taxonomic classification run, a quality value of $0$ is assumed. This value is not considered when calculating the weighted average quality.
                    }
                \end{enumerate}

                \paragraph{Results}
                The quality of the classification was calculated using the measure expressed by equation~\ref{Equation:Quality} for the implemented methods in comparison to the full taxonomic classification. Figure~\ref{Picture:Experiment:Quality} shows a graph of classification quality as a function of the number of input sequences, with detailed quality results provided in Table~\ref{Table:Experiment:Quality}. The weighted average quality calculated using equation~\ref{Equation:WeightedAverageQuality} for the method with the modified Needleman-Wunsch algorithm was $0.899$, for the $k$-mer embedding method it was $0.921$, and for the method using the artificial neural network, it was $0.944$.

                \begin{figure}[!htb]
                    \begin{center}
                        \includegraphics[width=\textwidth]{pictures/experiment_quality.png}
                    \end{center}
                    \caption{
                        Taxonomic classification quality.
                    }\label{Picture:Experiment:Quality}
                \end{figure}

                \begin{table}\centering
                    \caption{Taxonomic classification quality.}\label{Table:Experiment:Quality}

                    \begin{tabularx}{\textwidth}{|c|R|R|R|}
                        \hline
                        \multirow{2}{*}{\textbf{Number of sequences}} & \multicolumn{3}{|c|}{\textbf{Method}} \\ \cline{2-4}
                        & \textbf{NW} & \textbf{$k$-mer} & \textbf{ANN} \\ \hline \hline
                        1 & \textbf{1.0} & \textbf{1.0} & \textbf{1.0}\\ \hline
                        2 & 0.27 & \textbf{0.73} & \textbf{0.73}\\ \hline
                        4 & \textbf{0.8} & 0.2 & 0.2\\ \hline
                        8 & \textbf{0.94} & 0.88 & 0.12\\ \hline
                        16 & 0.82 & 0.82 & \textbf{0.94}\\ \hline
                        32 & 0.88 & \textbf{0.92} & 0.87\\ \hline
                        64 & 0.93 & \textbf{0.94} & 0.93\\ \hline
                        128 & 0.89 & 0.87 & \textbf{0.91}\\ \hline
                        256 & 0.89 & 0.89 & \textbf{0.93}\\ \hline
                        512 & 0.73 & 0.76 & \textbf{0.94}\\ \hline
                        1024 & \textbf{0.95} & 0.93 & \textbf{0.95}\\ \hline
                        2048 & 0.92 & \textbf{0.95} & 0.92\\ \hline
                        4096 & 0.0 & 0.93 & \textbf{0.96}\\ \hline
                    \end{tabularx}
                \end{table}

                \paragraph{Conclusions}
                All implemented methods achieved very good classification quality results, exceeding $0.7$ in cases where the number of groups was significantly larger than the number of input sequences. The weighted average quality for all methods also reached a high value.

                The method using the artificial neural network achieved the best weighted average quality, slightly outperforming the $k$-mer embedding method and the method with the modified Needleman-Wunsch algorithm. The artificial neural network method owes its result to the model's consideration of the full sequence structure, which allowed for a better determination of dissimilarity between sequences.

    \clearpage
    \section{Discussion}

        \subsection{Interpretation of Results}

            The results of the experiments partially confirmed the expectations set at the beginning of the research. All methods achieved satisfactory taxonomic classification quality. The method utilizing the artificial neural network performed particularly well, achieving the best weighted average classification quality while maintaining execution time comparable to the method based on $k$-mer embeddings. The low taxonomic classification quality with a small number of input sequences may be due to the inappropriate selection of group representatives, caused by too few available sequences. Therefore, the implemented methods should only be used in cases where the number of input sequences exceeds a certain threshold, and their use significantly reduces the time required for the taxonomic classification process. For the data tested, this condition is met for $128$ sequences, where the taxonomic classification quality was no less than $0.87$, and the use of any grouping method reduced the time by at least 5 minutes, resulting in a $5\%$ acceleration compared to the taxonomic classification of all sequences. The use of the artificial neural network allowed for an improvement in classification quality compared to other methods. In addition to achieving good classification quality, it does not lag behind classical methods.

        \subsection{Limitations}

            \subsubsection{Experiments Duration}
                The full process of taxonomic classification of DNA sequences for each method and each experimental subset took approximately 5 days, which led to the limitation of the number of number of experiments to single run and subset size to $4096$ sequences.

            \subsubsection{Single Dataset}  
                In work, only one sample from the chosen dataset was used. The selection of a sample containing DNA sequences from the human skin microbiome might have limited the space of analyzed sequences, which could have influenced the experimental results, as the both the artificial neural network and the experiments used DNA sequences from the same microbiome.

            \subsubsection{Single Dissimilarity Matrix}
                The use of a single dissimilarity matrix for all sequences limits the number of input sequences, as the matrix build time of matrix scales quadratically with the number of sequences.

        \subsection{Possible Improvements}

            \subsubsection{Architecture of Artificial Neural Network}

                The proposed artificial neural network architecture is not flexible and is only suitable for analyzing sequences of similar lengths. It would be possible to modify the model's architecture to incorporate recurrent neural networks (RNNs) and transformers, enabling the processing of sequences with varying lengths. RNNs and transformers could replace the first part of the model, which currently relies on convolutional layers. They would be used to create embeddings, which would later be processed by linear layers to generate an embedding that preserves dissimilarity properties.

            \subsubsection{Batching sequences}

                Creating a single dissimilarity matrix for very large samples is not optimal, as it significantly impacts the performance of the method. To address this issue, pre-clustering the sequences into batches can provide a solution. Processing in batches may reduce quality but allows for maintaining a short execution time.

            \subsubsection{Automatated Determination Of The Number of Groups}

                Current approach takes the number of created groups as a parameter, which can influence the quality of the results. For very similar sequences, a large number of groups is excessive and does not improve the results. Conversely, for highly diverse sequences, a low number of groups can reduce quality. Automating the determination of the optimal number of groups could solve the issue and minimize the need for fine-tuning 

    \clearpage
    \section{Conclusions and Future Work}

        \subsection{Summary of Findings}

            The experiments demonstrated that the method based on artificial neural networks outperforms traditional approaches in terms of quality, while maintaining execution times comparable to those of the classical $k$-mer method. This highlights the potential of neural networks for improving performance without significant computational overhead.

        \subsection{Future Research Directions}

            Future research could focus on utilizing artificial neural networks as an independent tool for taxonomic classification. Another promising direction is the application of artificial neural networks for dimensionality reduction of genetic sequences, which may improve efficiency and performance in various analyses.

\end{document}